{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent past, there has been a lot of research in language generation with auto-regressive models, like Transformers. In auto-regressive language generation, the probability distribution of token at time step K is dependent on the model's token-predictions till step K-1. For these models, decoding strategies such as Beam search, Greedy, Top-p and Top-k are critical components of the model and largely influence the style/nature of the generated output token at a given time step K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from official import nlp\n",
    "from official.nlp.modeling.ops import sampling_module, beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['num_heads'] = 2\n",
    "params['num_layers'] = 2\n",
    "params['batch_size'] = 2\n",
    "params['n_dims'] = 256\n",
    "params['max_decode_length'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In auto-regressive architectures like Transformers based Encoder-Decoder models, Cache is used for fast sequential decoding. It is a nested dictionary storing pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) for every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {\n",
    "    'layer_%d' % layer: {\n",
    "      'k': tf.zeros([params['batch_size'], params['max_decode_length'], params['num_heads'], int(params['n_dims']/params['num_heads'])], dtype=tf.float32),\n",
    "      'v': tf.zeros([params['batch_size'], params['max_decode_length'], params['num_heads'], int(params['n_dims']/params['num_heads'])], dtype=tf.float32),\n",
    "    } for layer in range(params['num_layers'])\n",
    "}\n",
    "\n",
    "print(\"cache key shape for layer 1:\", cache['layer_1']['k'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define closure for length normalization if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used for normalizing the final scores of generated sequences and is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_norm(length, dtype):\n",
    "  \"\"\"Return length normalization factor\"\"\"\n",
    "  return tf.pow(((5. + tf.cast(length, dtype)) / 6.), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, this will be replaced by an actial model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.constant([[[0.3, 0.4, 0.3], [0.3, 0.3, 0.4],\n",
    "                              [0.1, 0.1, 0.8], [0.1, 0.1, 0.8]],\n",
    "                            [[0.2, 0.5, 0.3], [0.2, 0.7, 0.1],\n",
    "                              [0.1, 0.1, 0.8], [0.1, 0.1, 0.8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(i):\n",
    "  return probabilities[:, i, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize symbols_to_logits_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _symbols_to_logits_fn():\n",
    "  \"\"\"Calculates logits of the next tokens\"\"\"\n",
    "  def symbols_to_logits_fn(ids, i, temp_cache):\n",
    "    del ids\n",
    "    logits = tf.cast(tf.math.log(model_fn(i)), tf.float32)\n",
    "    return logits, temp_cache\n",
    "  return symbols_to_logits_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoding selects the token id with the highest probability as its next id: id_t = argmax_wP(id|id_(1:t-1)) at each timestep t. The following sketch shows greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_obj = sampling_module.SamplingModule(\n",
    "    length_normalization_fn = None,\n",
    "    dtype = tf.float32,\n",
    "    symbols_to_logits_fn = _symbols_to_logits_fn(),\n",
    "    vocab_size = 3,\n",
    "    max_decode_length = params['max_decode_length'],\n",
    "    eos_id = 10,\n",
    "    padded_decode = False, #For TPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, _ = greedy_obj.generate(\n",
    "    initial_ids = tf.constant([9, 1]),\n",
    "    initial_cache = cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gredy Decoded Ids:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top_k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In top_k sampling, the K most likely next token ids are filtered and the probability mass is redistributed among only thos K ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_obj = sampling_module.SamplingModule(\n",
    "    length_normalization_fn = length_norm,\n",
    "    dtype = tf.float32,\n",
    "    symbols_to_logits_fn = _symbols_to_logits_fn(),\n",
    "    vocab_size = 3,\n",
    "    max_decode_length = params['max_decode_length'],\n",
    "    eos_id = 10,\n",
    "    sample_temperature = tf.constant(1.0),\n",
    "    top_k = tf.constant(3),\n",
    "    padded_decode = False,\n",
    "    enable_greedy = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, _ = top_k_obj.generate(\n",
    "    initial_ids = tf.constant([9, 1]),\n",
    "    initial_cache = cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top-k sampled Ids:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top_p sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sampling only from the most likely K token ids, in top_p sampling chooses from the smallest possible set of ids whose cumulative probability exceeds the probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_obj = sampling_module.SamplingModule(\n",
    "    length_normalization_fn = length_norm,\n",
    "    dtype = tf.float32,\n",
    "    symbols_to_logits_fn = _symbols_to_logits_fn(),\n",
    "    vocab_size = 3,\n",
    "    max_decode_length = params['max_decode_length'],\n",
    "    eos_id = 10,\n",
    "    sample_temperature = tf.constant(1.0),\n",
    "    top_p = tf.constant(0.9),\n",
    "    padded_decode = False,\n",
    "    enable_greedy = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, _ = top_p_obj.generate(\n",
    "    initial_ids = tf.constant([9, 1]),\n",
    "    initial_cache = cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top-p sampled Ids:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search reduces the risk of missing hidden high probability token ids by keeping the most likely num_beams of hypothesis at each time step and eventually choosing the hypothesis that has the overall highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 2\n",
    "\n",
    "params['batch_size'] = 1\n",
    "beam_cache = {\n",
    "    'layer_%d' % layer: {\n",
    "        'k': tf.zeros([params['batch_size'], params['max_decode_length'], params['num_heads'], params['n_dims']], dtype=tf.float32),\n",
    "        'v': tf.zeros([params['batch_size'], params['max_decode_length'], params['num_heads'], params['n_dims']], dtype=tf.float32)\n",
    "    } for layer in range(params['num_layers'])\n",
    "}\n",
    "\n",
    "print(\"cache key shape for layer 1 :\", beam_cache['layer_1']['k'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, _ = beam_search.sequence_beam_search(\n",
    "    symbols_to_logits_fn = _symbols_to_logits_fn(),\n",
    "    initial_ids = tf.constant([9], tf.int32),\n",
    "    initial_cache = beam_cache,\n",
    "    vocab_size = 3,\n",
    "    beam_size = beam_size,\n",
    "    alpha = 0.6,\n",
    "    max_decode_length = params['max_decode_length'],\n",
    "    eos_id = 10,\n",
    "    padded_decode = False,\n",
    "    dtype = tf.float32,\n",
    ")\n",
    "print(\"Beam search ids:\", ids)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMb2ShGfdIC1wG1oxxWdkig",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

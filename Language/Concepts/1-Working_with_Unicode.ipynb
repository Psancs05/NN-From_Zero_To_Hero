{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPL models often handle different languages with different character sets. Unicode is a standard encoding system that is udes to represent characters from almost all languages.\n",
    "Every Unicode character is encoded using a unique integer code point between 0 and 0x10FFFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(u\"Thanks üòä\").shape #Unicode strings are utf-8 encoded by defautl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two standard ways to represent a Unicode string in TF:\n",
    "*   string scalar - where the sequence of code points is encoded using a known character encoding.\n",
    "*   int32 vector - where each postion contains a single code point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following three values all represent the Unicode string \"ËØ≠Ë®ÄÂ§ÑÁêÜ\" (which means \"language processing\" in Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode string, represented as a UTF-8 encoded string scalar\n",
    "text_utf8 = tf.constant(u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\")\n",
    "text_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode string, represented as a UTF-16-BE encoded string scalar\n",
    "text_utf16be = tf.constant(u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\".encode(\"UTF-16-BE\"))\n",
    "text_utf16be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode string, represented as a vector of Unicode code points\n",
    "text_chars = tf.constant([ord(char) for char in u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\"])\n",
    "text_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF provides operations to convert between these different representations:\n",
    "*      tf.strings.unicode_decode: Converts an encoded string scalar toa vector of code points.\n",
    "*      tf.strings.unicode_encode: Converts a vector of code points to an encoded string scalar.\n",
    "*      tf.strings.unicode_transcode: Converts an ecoded string scalar to a different encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chars_converted = tf.strings.unicode_decode(text_utf8, input_encoding='UTF-8')\n",
    "assert tf.reduce_all(tf.equal(text_chars, text_chars_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utf8_converted = tf.strings.unicode_encode(text_chars, output_encoding='UTF-8')\n",
    "assert tf.reduce_all(tf.equal(text_utf8, text_utf8_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utf16be_converted = tf.strings.unicode_transcode(text_utf8, input_encoding='UTF-8', output_encoding='UTF-16-BE')\n",
    "assert tf.reduce_all(tf.equal(text_utf16be, text_utf16be_converted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When decoding multiple strings, the number of characters in each string may not be equal.\n",
    "The return result is a tf.RaggedTensor, where the innermost dimension length varies deoending in the number of characters in each string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A RaggedTensor is a tensor with one or more ragged dimensions, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []] is ragged, since the column slices (rt[0, :], ..., rt[4, :]) have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A batch of Unicode strings, each represented as a UTF8-encoded string\n",
    "batch_utf8 = [s.encode('UTF-8') for s in [u'h√Éllo', u'What is the weather tomorrow', u'G√∂√∂dnight', u'üòä']]\n",
    "batch_chars_ragged = tf.strings.unicode_decode(batch_utf8, input_encoding='UTF-8')\n",
    "\n",
    "for sentence_chars in batch_chars_ragged.to_list():\n",
    "  print(sentence_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)\n",
    "print(batch_chars_padded.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer way to represent the ragged tensor\n",
    "batch_chars_sparse = batch_chars_ragged.to_sparse()\n",
    "\n",
    "nrows, ncols = batch_chars_sparse.dense_shape.numpy()\n",
    "elements = [['_' for i in range(ncols)] for j in range(nrows)]\n",
    "for (row, col), value in zip(batch_chars_sparse.indices.numpy(), batch_chars_sparse.values.numpy()):\n",
    "  elements[row][col] = str(value)\n",
    "# max_width = max(len(value) for row in elements for value in row)\n",
    "value_lengths = []\n",
    "for row in elements:\n",
    "  for value in row:\n",
    "    value_lengths.append(len(value))\n",
    "max_width = max(value_lengths)\n",
    "print('[%s]' % '\\n '.join(\n",
    "    '[%s]' % ', '.join(value.rjust(max_width) for value in row)\n",
    "    for row in elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When encoding multiple strings with the same lenghts, use tf.Tensor as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_encode([[99, 97, 116], [100, 111, 103], [99, 111, 119]],\n",
    "                          output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When encoding multiple string with varying length, use tf.RaggedTensor as the input instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_encode(batch_chars_ragged, output_encoding='UTF-8')\n",
    "#Here batch_chars_ragged is a tf.RaggedTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have a tensor with multiple strings in padded or sparse format, convert it first into a tf.RaggedTensor before calling tf.string.enicode_encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_encode(tf.RaggedTensor.from_sparse(batch_chars_sparse), output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_encode(tf.RaggedTensor.from_tensor(batch_chars_padded, padding=-1), output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the unit parameter of the tf.strings.lenght op to indicate how character lengths should be computed. unit defaults to \"BYTE\", but it can be set to other values, such as \"UTF8_CHAR\" or \"UTF16_CHAR\", to determine the number of Unicode codepoints in each encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Note that the final character (emoji) takes up 4 bytes in UTF8.\n",
    "  thanks = u'Thanks üòä'.encode('UTF-8')\n",
    "  print(thanks)\n",
    "  num_bytes = tf.strings.length(thanks).numpy()\n",
    "  num_chars = tf.strings.length(thanks, unit='UTF8_CHAR').numpy()\n",
    "  print('{} bytes; {} UTF-8 characters'.format(num_bytes, num_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.strings.substr op accepts the unit parameter, and uses it to determine what kind of offsets the pos and len parameters contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, unit='BYTE' (default). Returns a single byte (position 7) with len=1\n",
    "tf.strings.substr(thanks, pos=7, len=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying unit='UTF8_CHAR', returns a single 4 byte character (emoji) in this case\n",
    "tf.strings.substr(thanks, pos=7, len=1, unit='UTF8_CHAR').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Unicode strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.strings.unicode_split op splits Unicode strings into substrings of individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_split(thanks, input_encoding='UTF-8').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte iffsets for characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the character tensor generated by tf.strings.unicode_decode with the original string, it is useful to know the offset for where each character begins. The method tf.strings.unicode_decode_with_offsets is similar to unicode_decode, except that it returns a second tensor containing the start offset of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepoints, offsets = tf.strings.unicode_decode_with_offsets(u'üéàüéâüéä', 'UTF-8')\n",
    "\n",
    "print(codepoints.numpy())\n",
    "print(offsets.numpy())\n",
    "\n",
    "for (codepoint, offset) in zip(codepoints.numpy(), offsets.numpy()):\n",
    "  print('At byte offset {}: codepoints {}'.format(offset, codepoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Unicode code point belongs to a single collection of codepoints known as a script. A character's script i helpful in determining which language the character might be in.\n",
    "<br>TF provides the tf.strings.unicode_script op to determine which script a given codepoint uses. The script codes are in int32 values corresponding to ICU [UScriptCode](https://unicode-org.github.io/icu-docs/apidoc/released/icu4c/uscript_8h.html) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscript = tf.strings.unicode_script([33464, 1041]) # ['Ëä∏', '–ë']\n",
    "print(uscript.numpy()) # [17, 8] == [USCRIPT_HAN, USCRIPT_CYRILLIC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.strings.unicode_script op can also be applied to multidimensional tf.Tensor's or tf.RaggedTensor's of codepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strings.unicode_script(batch_chars_ragged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Simple segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation is the task of splitting text into word-like units. This is often easy when space characters are used to separate words, but some languages (like Chinese and Japanese) do not use spaces, and some languages (like German) contain long compounds that must be split in order to analyze their meaning. In web text, different languages and scripts are frequently mixed together, as in \"NYÊ†™‰æ°\" (New York Stock Exchange).\n",
    "<br>We can perform very rough segmentation (without implementing any ML models) by using changes in script to approximate word boundaries. This will work for strings like the \"NYÊ†™‰æ°\" example above. It will also work for most languages that use spaces, as the space characters of various scripts are all classified as USCRIPT_COMMON, a special script code that differs from that of any actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype: string; shape: [num_sentences]\n",
    "\n",
    "# The sentences to process.  Edit this line to try out different inputs!\n",
    "sentence_texts = [u'Hello, world.', u'‰∏ñÁïå„Åì„Çì„Å´„Å°„ÅØ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, decode the sentences into character codepoints, and find the script identifier for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_char_codepoint = tf.strings.unicode_decode(sentence_texts, input_encoding='UTF-8')\n",
    "print(sentence_char_codepoint)\n",
    "\n",
    "sentence_char_script = tf.strings.unicode_script(sentence_char_codepoint)\n",
    "print(sentence_char_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the script identifiers to determine where word boundaries should be added. Add a word boundary at the beggining of each sentence, and for each character whose script differs from the previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_char_starts_word = tf.concat(\n",
    "    [\n",
    "    tf.fill([sentence_char_script.nrows(), 1], True),\n",
    "    tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])\n",
    "    ], axis=1\n",
    ")\n",
    "print(sentence_char_starts_word)\n",
    "\n",
    "word_starts = tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)\n",
    "print(word_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use those start offsets to build a RaggedTensor containing the list of words from all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_char_codepoint = tf.RaggedTensor.from_row_starts(\n",
    "    values = sentence_char_codepoint.values,\n",
    "    row_starts = word_starts,\n",
    ")\n",
    "print(word_char_codepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish, segment the word codepoints RaggedTensor back into sentences and encoed into UTF-8 strings for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_num_words = tf.reduce_sum(\n",
    "    tf.cast(sentence_char_starts_word, tf.int64),\n",
    "    axis=1,\n",
    ")\n",
    "print(sentence_num_words)\n",
    "\n",
    "sentence_word_char_codepoint = tf.RaggedTensor.from_row_lengths(\n",
    "    values = word_char_codepoint,\n",
    "    row_lengths = sentence_num_words,\n",
    ")\n",
    "print(sentence_word_char_codepoint)\n",
    "\n",
    "tf.strings.unicode_encode(sentence_word_char_codepoint, output_encoding='UTF-8').to_list()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMtgVnGF+nV+iTiR8kB65qN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

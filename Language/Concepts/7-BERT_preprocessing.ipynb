{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is the end-to-end transformation of raw text into a modelâ€™s integer inputs. NLP models are often accompanied by several hundreds (if not thousands) of lines of Python code for preprocessing text. Text preprocessing is often a challenge for models because:\n",
    "\n",
    "*      Training-serving skew\n",
    "*      Efficiency and flexibility\n",
    "*      Complex model interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text preprocessing with *TF.Text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF.Text's text preprocessing APIs, we can construct a preprocessing function that can transform a user's text dataset into the model's integer inputs. Users can package preprocessing directly as part of their model to alleviate the above mentioned problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show how to use TF.Text preprocessing ops to transform text data into inputs for the BERT model and inputs for language masking pretraining task described in \"Masked LM and Masking Procedure\" of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. The process involves tokenizing text into subword units, combining sentences, trimming content to a fixed size and extracting labels for the masked language modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains two text features and we can create a example tf.data.Dataset. Our goal is to create a function that we can supply Dataset.map() with to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to run any string preprocessing and tokenize our dataset. This can be done using the text.BertTokenizer, which is a *text.Splitter* that can tokenize sentences into subwords or wordpieces for the BERT model given a vocabulary generated from the Wordpiece algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys = _VOCAB,\n",
    "        key_dtype = tf.string,\n",
    "        values = tf.range(\n",
    "            tf.size(_VOCAB, out_type = tf.int64), dtype = tf.int64),\n",
    "            value_dtype = tf.int64,\n",
    "        ),\n",
    "        num_oov_buckets = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a *text.BertTokenizer* using the above vocabulary and tokenize the text inputs into a RaggedTensor.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize(examples[\"text_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize(examples[\"text_b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text output from *text.BertTokenizer* allows us see how the text is being tokenized, but the model requires integer IDs. We can set the token_out_type param to tf.int64 to obtain integer IDs (which are the indices into the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_a = bert_tokenizer.tokenize(examples[\"text_a\"])\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_b = bert_tokenizer.tokenize(examples[\"text_b\"])\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*text.BertTokenizer* returns a RaggedTensor with shape [batch, num_tokens, num_wordpieces]. Because we don't need the extra num_tokens dimensions for our current use case, we can merge the last two dimensions to obtain a RaggedTensor with shape [batch, num_wordpieces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_a = segment_a.merge_dims(-2, -1)\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_b = segment_b.merge_dims(-2, -1)\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main input to BERT is a concatenation of two sentences. However, BERT requires inputs to be in a fixed-size and shape and we may have content which exceed our budget.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tackle this by using a *text.Trimmer* to trim our content down to a predetermined size (once concatenated along the last axis). There are different *text.Trimmer* types which select content to preserve using different algorithms. *text.RoundRobinTrimmer* for example will allocate quota equally for each segment but may trim the ends of sentences. *text.WaterfallTrimmer* will trim starting from the end of the last sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will use RoundRobinTrimmer which selects items from each segment in a left-to-right manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = text.RoundRobinTrimmer(max_seq_length = _MAX_SEQ_LEN)\n",
    "trimmed = trimmer.trim([segment_a, segment_b])\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trimmed now contains the segments where the number of elements across a batch is 8 elements (when concatenated along axis=-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have segments trimmed, we can combine them together to get a single RaggedTensor. BERT uses special tokens to indicate the beginning ([CLS]) and end of a segment ([SEP]). We also need a RaggedTensor indicating which items in the combined Tensor belong to which segment. We can use text.combine_segments() to get both of these Tensor with special tokens inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_combined, segments_ids = text.combine_segments(\n",
    "    trimmed,\n",
    "    start_of_sequence_id = _START_TOKEN,\n",
    "    end_of_segment_id = _END_TOKEN,\n",
    ")\n",
    "segments_combined, segments_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Model Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our basic inputs, we can begin to extract the inputs needed for the \"Masked LM and Masking Procedure\" task. It has two sub-problems for us to think about: (1) what items to select for masking and (2) what values are they assigned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will choose to select items randomly for masking, we will use a *text.RandomItemSelector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch = _MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate = 0.2,\n",
    "    unselectable_ids = [_START_TOKEN, _END_TOKEN, _UNK_TOKEN],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = random_selector.get_selection_mask(\n",
    "    segments_combined, axis=1,\n",
    ")\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Masked Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology described the original BERT paper for choosing the value for masking is as follows:\n",
    "\n",
    "*      For mask_token_rate of the time, replace the item with the [MASK] token:\n",
    "\n",
    "\"my dog is hairy\" -> \"my dog is [MASK]\"\n",
    "\n",
    "*      For random_token_rate of the time, replace the item with a random word:\n",
    "\n",
    "\"my dog is hairy\" -> \"my dog is apple\"\n",
    "\n",
    "*      For 1 - mask_token_rate - random_token_rate of the time, keep the item unchanged:\n",
    "\n",
    "\"my dog is hairy\" -> \"my dog is hairy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*text.MaskedValuesChooser* encapsulates this logic and can be used for our preprocessing function. Here's an example of what MaskValuesChooser returns given a mask_token_rate of 80% and default random_token_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
    "mask_values_chooser.get_mask_values(segments_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Inpust for Masked Language Model Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a RandomItemSelector to help us select items for masking and *text.MaskValuesChooser* to assign the values, we can use *text.mask_language_model()* to assemble all the inputs of this task for our BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_token_ids, masked_pos, masked_lm_ids = text.mask_language_model(\n",
    "    segments_combined,\n",
    "    item_selector = random_selector,\n",
    "    mask_values_chooser = mask_values_chooser,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive deeper and examine the outputs of *mask_language_model()*. The output of masked_token_ids is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our input is encoded using a vocabulary. If we decode masked_token_ids using our vocabulary, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather(_VOCAB, masked_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some wordpiece tokens have been replaced with either [MASK], [RANDOM] or a different ID value. masked_pos output gives us the indices (in the respective batch) of the tokens that have been replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked_lm_ids gives us the original value of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again decode the IDs here to get human readable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather(_VOCAB, masked_lm_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the inputs for our model, the last step in our preprocessing is to package them into fixed 2-dimensional Tensors with padding and also generate a mask Tensor indicating the values which are pad values. We can use *text.pad_model_inputs()* to help us with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and pad combined segment inputs\n",
    "input_word_ids, input_mask = text.pad_model_inputs(\n",
    "  masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "input_type_ids, _ = text.pad_model_inputs(\n",
    "  segments_ids, max_seq_length=_MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and pad masking task inputs\n",
    "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "  masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "masked_lm_ids, _ = text.pad_model_inputs(\n",
    "  masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"input_word_ids\": input_word_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "    \"masked_lm_ids\": masked_lm_ids,\n",
    "    \"masked_lm_positions\": masked_lm_positions,\n",
    "    \"masked_lm_weights\": masked_lm_weights,\n",
    "}\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review what we have so far and assemble our preprocessing function. Here's what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "  # Input is a string Tensor of documents, shape [batch, 1].\n",
    "  text_a = features[\"text_a\"]\n",
    "  text_b = features[\"text_b\"]\n",
    "\n",
    "  # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
    "  tokenizer = text.BertTokenizer(\n",
    "      vocab_table,\n",
    "      token_out_type=tf.int64)\n",
    "  segments = [tokenizer.tokenize(text).merge_dims(\n",
    "      1, -1) for text in (text_a, text_b)]\n",
    "\n",
    "  # Truncate inputs to a maximum length.\n",
    "  trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "  trimmed_segments = trimmer.trim(segments)\n",
    "\n",
    "  # Combine segments, get segment ids and add special tokens.\n",
    "  segments_combined, segment_ids = text.combine_segments(\n",
    "      trimmed_segments,\n",
    "      start_of_sequence_id=_START_TOKEN,\n",
    "      end_of_segment_id=_END_TOKEN)\n",
    "\n",
    "  # Apply dynamic masking task.\n",
    "  masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "      text.mask_language_model(\n",
    "        segments_combined,\n",
    "        random_selector,\n",
    "        mask_values_chooser,\n",
    "      )\n",
    "  )\n",
    "\n",
    "  # Prepare and pad combined segment inputs\n",
    "  input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "  input_type_ids, _ = text.pad_model_inputs(\n",
    "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "  # Prepare and pad masking task inputs\n",
    "  masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "  masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "  model_inputs = {\n",
    "      \"input_word_ids\": input_word_ids,\n",
    "      \"input_mask\": input_mask,\n",
    "      \"input_type_ids\": input_type_ids,\n",
    "      \"masked_lm_ids\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_lm_positions,\n",
    "      \"masked_lm_weights\": masked_lm_weights,\n",
    "  }\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously constructed a *tf.data.Dataset* and we can now use our assembled preprocessing function *bert_pretrain_preprocess()* in *Dataset.map()*. This allows us to create an input pipeline for transforming our raw string data into integer inputs and feed directly into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "next(iter(dataset))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMn7r5LTdUN9v3YG6r+GKyH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
